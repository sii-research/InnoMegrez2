<div align="center">

  <a href="https://huggingface.co/sii-research/InnoMegrez2">
    <b>ğŸ¤— Hugging Face</b>
  </a> &nbsp;|&nbsp;
  <a href="./docs/tech_report.pdf">
    <b>ğŸ“„ Tech Report</b>
  </a> &nbsp;|&nbsp;
  <a href="./assets/wechat-official.jpg">
    <b>ğŸ’¬ WeChat Official</b>
  </a> &nbsp;

  <br>

  <strong>ä¸­æ–‡ | [English](./README_EN.md)</strong>

</div>

# æ›´æ–°æ—¥å¿—

- [2025.09.15] å‘å¸ƒ [InnoMegrez2](https://github.com/sii-research/InnoMegrez2) æ­£å¼ç‰ˆæœ¬ï¼Œæ¨¡å‹ç»“æ„å’Œé¢„è§ˆç‰ˆæœ¬ä¸€è‡´ï¼Œè®­ç»ƒæ•°æ®æ€»é‡ä»5Tå¢åŠ åˆ°8Tï¼Œåœ¨å„ä¸ªæµ‹è¯•é›†ä¸Šè¡¨ç°æ›´åŠ å‡è¡¡ã€‚
- [2025.07.24] å‘å¸ƒ [InnoMegrez2-Preview](https://github.com/sii-research/InnoMegrez2) é¢„è§ˆç‰ˆæœ¬ï¼Œä¸“ä¸ºç»ˆç«¯è®¾å¤‡è®¾è®¡çš„å¤§æ¨¡å‹ï¼Œå…¼é¡¾MoEçš„ç²¾åº¦æ æ†ä¸Denseçš„æ€»å‚æ•°é‡å‹å¥½ã€‚


# InnoMegrez2

## æ¨¡å‹ç®€ä»‹

InnoMegrez2 æ˜¯ä¸“ä¸ºç»ˆç«¯è®¾å¤‡è®¾è®¡çš„å¤§æ¨¡å‹ï¼Œå…¼é¡¾MoEçš„ç²¾åº¦æ æ†ä¸Denseçš„æ€»å‚æ•°é‡å‹å¥½ã€‚æœ¬æ¬¡å‘å¸ƒçš„ä¸ºMegrez 2.0æ­£å¼ç‰ˆæœ¬ï¼Œè®­ç»ƒæ•°æ®é‡8T Tokensï¼Œæœªæ¥æˆ‘ä»¬è®¡åˆ’æå‡æ¨¡å‹çš„æ¨ç†å’ŒAgentèƒ½åŠ›ã€‚

## åŸºç¡€ä¿¡æ¯

<div align="center">

| | |
|:---:|:---:|
| **Architecture** | Mixture-of-Experts (MoE) |
| **Total Parameters** | 3x7B |
| **Activated Parameters** | 3B |
| **Experts Shared Frequency**| 3 |
| **Number of Layers** (Dense layer included) | 31 |
| **Number of Dense Layers** | 1 |
| **Attention Hidden Dimension** | 2048 |
| **MoE Hidden Dimension** (per Expert) | 1408 |
| **Number of Attention Heads** | 16 |
| **Number of Experts** | 64 |
| **Selected Experts per Token** | 6 |
| **Number of Shared Experts** | 4 |
| **Vocabulary Size** | 128,880 |
| **Context Length** | 32K |
| **Base Frequency of RoPE** | 1,000,000 |
| **Attention Mechanism** | GQA |
| **Activation Function** | SwiGLU |
</div>

## æ€§èƒ½æµ‹è¯•

æˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· [OpenCompass](https://github.com/open-compass/opencompass) å¯¹ InnoMegrez2 è¿›è¡Œäº†è¯„æµ‹ï¼Œéƒ¨åˆ†è¯„æµ‹ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

<div align="center">
<table>
<thead>
<tr>
<th align="center">Benchmark</th>
<th align="center">Metric</th>
<th align="center"><sup>InnoMegrez2<br></sup></th>
<th align="center"><sup>InnoMegrez2<br>-Preview</sup></th>
<th align="center"><sup>SmallThinker-21B<br>-A3B-Instruct</sup></th>
<th align="center"><sup>Qwen3-30B-A3B</sup></th>
<th align="center"><sup>Qwen3-8B</sup></th>
<th align="center"><sup>Qwen3-4B<br>-Instruct-2507</sup></th>
<th align="center"><sup>Phi4-14B<br>(nothink)</sup></th>
<th align="center"><sup>Gemma3-12B</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Activate Params (B)</td>
<td align="center"></td>
<td align="center">3.0</td>
<td align="center">3.0</td>
<td align="center">3.0</td>
<td align="center">3.3</td>
<td align="center">8.2</td>
<td align="center">4.0</td>
<td align="center">14.7</td>
<td align="center">12.2</td>
</tr>
<tr>
<td align="center">Stored Params (B)</td>
<td align="center"></td>
<td align="center">7.5</td>
<td align="center">7.5</td>
<td align="center">21.5</td>
<td align="center">30.5</td>
<td align="center">8.2</td>
<td align="center">4.0</td>
<td align="center">14.7</td>
<td align="center">12.2</td>
</tr>
<tr>
<td align="center">MMLU</td>
<td align="center">EM</td>
<td align="center">85.4</td>
<td align="center"><strong>87.5</strong></td>
<td align="center">84.4</td>
<td align="center">85.1</td>
<td align="center">81.8</td>
<td align="center">-</td>
<td align="center">84.6</td>
<td align="center">78.5</td>
</tr>
<tr>
<td align="center">GPQA</td>
<td align="center">EM</td>
<td align="center"><strong>58.8</strong></td>
<td align="center">28.8</td>
<td align="center">55.0</td>
<td align="center">44.4</td>
<td align="center">38.9</td>
<td align="center">62</td>
<td align="center">55.5</td>
<td align="center">34.9</td>
</tr>
<tr>
<td align="center">IFEval</td>
<td align="center">Inst<br>loose</td>
<td align="center"><strong>87.7</strong></td>
<td align="center">80.2</td>
<td align="center">85.8</td>
<td align="center">84.3</td>
<td align="center">83.9</td>
<td align="center">83.4</td>
<td align="center">63.2</td>
<td align="center">74.7</td>
</tr>
<tr>
<td align="center">MATH-500</td>
<td align="center">EM</td>
<td align="center"><strong>87.2</strong></td>
<td align="center">81.6</td>
<td align="center">82.4</td>
<td align="center">84.4</td>
<td align="center">81.6</td>
<td align="center">-</td>
<td align="center">80.2</td>
<td align="center">82.4</td>
</tr>
</tbody>
</table>
</div>

## å¦‚ä½•è¿è¡Œ

### Transformers

æ¨èä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ `transformers` æˆ–è€… `transformers>=4.52.4` çš„ç‰ˆæœ¬ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ä»£ç ç‰‡æ®µç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•è¿è¡Œ InnoMegrez2 æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

path = "sii-research/InnoMegrez2"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)

messages = [
    {"role": "user", "content": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§ï¼Ÿ"},
]
model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)

model_outputs = model.generate(
    model_inputs,
    do_sample=True,
    max_new_tokens=1024
)

output_token_ids = [
    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
]

responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]
print(responses)

# ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰çš„ä¸­å°¼è¾¹å¢ƒã€‚ç ç©†æœ—ç›å³°çš„æµ·æ‹”é«˜åº¦ä¸º8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ï¼Œè¿™ä¸€æ•°æ®æ˜¯ç”±ä¸­å›½å’Œå°¼æ³Šå°”åœ¨2020å¹´å…±åŒå®£å¸ƒçš„æœ€æ–°æµ‹é‡ç»“æœã€‚ç ç©†æœ—ç›å³°ä¸ä»…æ˜¯ç™»å±±çˆ±å¥½è€…çš„åœ£åœ°ï¼Œä¹Ÿæ˜¯åœ°ç†å’Œç§‘å­¦ç ”ç©¶çš„é‡è¦å¯¹è±¡ã€‚
```

## å¦‚ä½•éƒ¨ç½²

### vLLM

éœ€ä½¿ç”¨ `vllm>=0.10.1` ç‰ˆæœ¬ã€‚åœ¨å½“å‰ç‰ˆæœ¬ç¯å¢ƒä¸‹ï¼Œéœ€å¯¹ vllm ç›¸å…³æ–‡ä»¶è¿›è¡Œä¸€æ¬¡è¡¥ä¸æ›¿æ¢ï¼›åç»­æˆ‘ä»¬å°†æäº¤ pull requestï¼Œå°½æ—©å°†è¯¥ä¿®æ”¹åˆå¹¶è‡³ vllm çš„æ­£å¼ç‰ˆæœ¬ä¸­ã€‚

1. æ‰¾åˆ°ä½ çš„vllmå®‰è£…è·¯å¾„
```python
import vllm
print(vllm.__file__)
```

2. æ›¿æ¢vllmç›¸å…³æ–‡ä»¶
```shell
cp ./demo/vllm/patch/layer.py <vllm_install_path>/model_executor/layers/fused_moe/
```

#### vLLM ç¦»çº¿
```shell
cd demo/vllm
export MODEL_PATH="sii-research/InnoMegrez2"
python3 infer_vllm_offline.py $MODEL_PATH
```
#### vLLM åœ¨çº¿
åœ¨ç»ˆç«¯ä¸­å¯åŠ¨vLLMæœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹
```shell
cd demo/vllm
export MODEL_PATH="sii-research/InnoMegrez2"
python3 serve_llm_online.py serve $MODEL_PATH --gpu-memory-utilization 0.9 --served-model-name megrez-moe --trust_remote_code
```

ç°åœ¨ï¼Œå¯ä»¥é€šè¿‡curlå‘é€è¯·æ±‚
```shell
curl --location 'http://localhost:8000/v1/chat/completions' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer sk-123456' \
--data '{
    "model": "megrez-moe",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§ï¼Ÿ"
                }
            ]
        }
    ]
}'
```

### SGLang

æ¨è `sglang>=0.4.9.post2` çš„ç‰ˆæœ¬
```shell
cd demo/sglang
export MODEL_PATH="sii-research/InnoMegrez2" 
python3 infer_sglang_offline.py $MODEL_PATH
```


## æœ€ä½³å®è·µ

ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œå»ºè®®ä»¥ä¸‹è®¾ç½®ï¼š

1. é‡‡æ ·å‚æ•°ï¼šæ¨èä½¿ç”¨ Temperature=0.7 å’Œ TopP=0.9 ã€‚

2. æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼šåœ¨åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æç¤ºæ¥æ ‡å‡†åŒ–æ¨¡å‹è¾“å‡ºï¼Œæ¯”å¦‚ï¼š
    * æ•°å­¦é—®é¢˜ï¼šåœ¨æç¤ºä¸­åŒ…å«â€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨\boxed{}ä¸­ã€‚â€
    * é€‰æ‹©é¢˜ï¼šåœ¨æç¤ºä¸­æ·»åŠ ä»¥ä¸‹ JSON ç»“æ„ä»¥æ ‡å‡†åŒ–å“åº”ï¼šâ€œè¯·åœ¨ answer å­—æ®µä¸­ä»…ä»¥é€‰æ‹©å­—æ¯çš„å½¢å¼æ˜¾ç¤ºæ‚¨çš„é€‰æ‹©ï¼Œä¾‹å¦‚ "answer": "C" ã€‚â€

# è®¸å¯å£°æ˜

æˆ‘ä»¬æ‰€æœ‰çš„å¼€æºæ¨¡å‹å‡é‡‡ç”¨Apache 2.0åè®®æˆæƒã€‚


# è”ç³»æˆ‘ä»¬

å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶æäº¤GitHub issueæˆ–è”ç³»[å¾®ä¿¡ç¾¤ç»„](./assets/wechat-group.jpg)ã€‚