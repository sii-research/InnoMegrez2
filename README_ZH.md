<div align="center">

  <br>

  <a href="https://huggingface.co/sii-research/InnoMegrez2-Preview">
    <b>ğŸ¤— Hugging Face</b>
  </a> &nbsp;|&nbsp;
  <a href="./docs/tech_report.pdf">
    <b>ğŸ“„ Tech Report</b>
  </a> &nbsp;|&nbsp;
  <a href="./assets/wechat-official.jpg">
    <b>ğŸ’¬ WeChat Official</b>
  </a> &nbsp;

  <br>

  <strong>ä¸­æ–‡ | [English](./README.md)</strong>

</div>


# InnoMegrez2-Preview

## æ¨¡å‹ç®€ä»‹

InnoMegrez2-Preview æ˜¯ä¸“ä¸ºç»ˆç«¯è®¾å¤‡è®¾è®¡çš„å¤§æ¨¡å‹ï¼Œå…¼é¡¾MoEçš„ç²¾åº¦æ æ†ä¸Denseçš„æ€»å‚æ•°é‡å‹å¥½ã€‚æœ¬æ¬¡å‘å¸ƒçš„ä¸ºMegrez 2.0é¢„è§ˆç‰ˆæœ¬ï¼Œè®­ç»ƒæ•°æ®é‡5T Tokensï¼Œæœªæ¥æˆ‘ä»¬è®¡åˆ’å®Œæˆæ›´å¤§è§„æ¨¡çš„æ•°æ®è®­ç»ƒï¼Œå¹¶æé«˜æ¨¡å‹çš„æ¨ç†å’ŒAgentèƒ½åŠ›ï¼Œæ­£å¼ç‰ˆæœ¬é¢„è®¡ä»Šå¹´å¹´å†…å‘å¸ƒã€‚

## åŸºç¡€ä¿¡æ¯

<div align="center">

| | |
|:---:|:---:|
| **Architecture** | Mixture-of-Experts (MoE) |
| **Total Parameters** | 3x7B |
| **Activated Parameters** | 3B |
| **Experts Shared Frequency**| 3 |
| **Number of Layers** (Dense layer included) | 31 |
| **Number of Dense Layers** | 1 |
| **Attention Hidden Dimension** | 2048 |
| **MoE Hidden Dimension** (per Expert) | 1408 |
| **Number of Attention Heads** | 16 |
| **Number of Experts** | 64 |
| **Selected Experts per Token** | 6 |
| **Number of Shared Experts** | 4 |
| **Vocabulary Size** | 128,880 |
| **Context Length** | 32K |
| **Base Frequency of RoPE** | 1,000,000 |
| **Attention Mechanism** | GQA |
| **Activation Function** | SwiGLU |
</div>

## æ€§èƒ½æµ‹è¯•

æˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· [OpenCompass](https://github.com/open-compass/opencompass) å¯¹ Megrez2-3x7B-A3B-Preview è¿›è¡Œäº†è¯„æµ‹ï¼Œéƒ¨åˆ†è¯„æµ‹ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

<div align="center">
<table>
<thead>
<tr>
<th align="center">Benchmark</th>
<th align="center">Metric</th>
<th align="center"><sup>InnoMegrez2-Preview</sup></th>
<th align="center"><sup>Qwen2.5-3B</sup></th>
<th align="center"><sup>Qwen2.5-7B</sup></th>
<th align="center"><sup>Qwen3-4B</sup></th>
<th align="center"><sup>Qwen3-8B</sup></th>
<th align="center"><sup>Phi-4-mini</sup></th>
<th align="center"><sup>Gemma-3-4B</sup></th>
<th align="center"><sup>GPT-4o-mini <br><sup>2024-07-18</sup></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Activate Params (B)</td>
<td align="center"></td>
<td align="center">3.0</td>
<td align="center">3.1</td>
<td align="center">7.6</td>
<td align="center">4.0</td>
<td align="center">8.2</td>
<td align="center">3.8</td>
<td align="center">4.3</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">Stored Params (B)</td>
<td align="center"></td>
<td align="center">7.5</td>
<td align="center">3.1</td>
<td align="center">7.6</td>
<td align="center">4.0</td>
<td align="center">8.2</td>
<td align="center">3.8</td>
<td align="center">4.3</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center" colspan=9><strong>General Tasks</strong></td>
</tr>
<tr>
<td align="center">C-EVAL</td>
<td align="center">EM</td>
<td align="center"><strong>91.7</strong></td>
<td align="center">68.2</td>
<td align="center">76.2</td>
<td align="center">72.2</td>
<td align="center">77.9</td>
<td align="center">40.0</td>
<td align="center">-</td>
<td align="center">66.3</td>
</tr>
<tr>
<td align="center">MMLU-Pro</td>
<td align="center">EM</td>
<td align="center"><strong>67.6</strong></td>
<td align="center">43.7</td>
<td align="center">56.3</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">52.8</td>
<td align="center">43.6</td>
<td align="center">-</td>
</tr>
<td align="center" colspan=9><strong>Instruction Tasks</strong></td>
<tr>
<td align="center">IF-Eval</td>
<td align="center">Prompt Strict</td>
<td align="center">80.2</td>
<td align="center">58.2</td>
<td align="center">71.2</td>
<td align="center">81.2</td>
<td align="center">83.0</td>
<td align="center">68.6</td>
<td align="center"><strong>90.2</strong></td>
<td align="center">80.4</td>
</tr>
<td align="center" colspan=9><strong>Math & STEM Tasks</strong></td>
<tr>
<td align="center">MATH-500</td>
<td align="center">EM</td>
<td align="center">81.6</td>
<td align="center">65.9</td>
<td align="center">75.5</td>
<td align="center">84.8</td>
<td align="center"><strong>87.4</strong></td>
<td align="center">64.0</td>
<td align="center">75.6</td>
<td align="center">78.2</td>
</tr>
<tr>
<td align="center">GSM8K</td>
<td align="center">EM</td>
<td align="center">83.6</td>
<td align="center">86.7</td>
<td align="center">91.6</td>
<td align="center">-</td>
<td align="center"><strong>93.2</strong></td>
<td align="center">88.6</td>
<td align="center">89.2</td>
<td align="center">-</td>
</tr>
<td align="center" colspan=9><strong>Coding Tasks</strong></td>
<tr>
<td align="center">HumanEval</td>
<td align="center">Pass@1</td>
<td align="center">74.4</td>
<td align="center">74.4</td>
<td align="center">84.8</td>
<td align="center">-</td>
<td align="center"><strong>85.9</strong></td>
<td align="center">74.4</td>
<td align="center">71.3</td>
<td align="center">87.2</td>
</tr>
<tr>
<td align="center">MBPP</td>
<td align="center">Pass@1</td>
<td align="center"><strong>88.0</strong></td>
<td align="center">72.7</td>
<td align="center">79.2</td>
<td align="center">-</td>
<td align="center">77.0</td>
<td align="center">65.3</td>
<td align="center">63.2</td>
<td align="center">-</td>
</tr>
</tbody>
</table>
</div>

## å¦‚ä½•è¿è¡Œ

### Transformers

æ¨èä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ `transformers` æˆ–è€… `transformers>=4.52.4` çš„ç‰ˆæœ¬ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ä»£ç ç‰‡æ®µç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•è¿è¡Œ InnoMegrez2-Preview æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

path = "sii-research/InnoMegrez2-Preview"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)

messages = [
    {"role": "user", "content": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§ï¼Ÿ"},
]
model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)

model_outputs = model.generate(
    model_inputs,
    do_sample=True,
    max_new_tokens=1024
)

output_token_ids = [
    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
]

responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]
print(responses)

# ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰çš„ä¸­å°¼è¾¹å¢ƒã€‚ç ç©†æœ—ç›å³°çš„æµ·æ‹”é«˜åº¦ä¸º8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ï¼Œè¿™ä¸€æ•°æ®æ˜¯ç”±ä¸­å›½å’Œå°¼æ³Šå°”åœ¨2020å¹´å…±åŒå®£å¸ƒçš„æœ€æ–°æµ‹é‡ç»“æœã€‚ç ç©†æœ—ç›å³°ä¸ä»…æ˜¯ç™»å±±çˆ±å¥½è€…çš„åœ£åœ°ï¼Œä¹Ÿæ˜¯åœ°ç†å’Œç§‘å­¦ç ”ç©¶çš„é‡è¦å¯¹è±¡ã€‚
```

## å¦‚ä½•éƒ¨ç½²

### vLLM

æ¨è `vllm>=0.9.2` çš„ç‰ˆæœ¬

#### vLLM ç¦»çº¿
```shell
cd demo/vllm
export MODEL_PATH="sii-research/InnoMegrez2-Preview"
python3 infer_vllm_offline.py $MODEL_PATH
```
#### vLLM åœ¨çº¿
åœ¨ç»ˆç«¯ä¸­å¯åŠ¨vLLMæœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹
```shell
cd demo/vllm
export MODEL_PATH="sii-research/InnoMegrez2-Preview"
python3 serve_llm_online.py serve $MODEL_PATH --gpu-memory-utilization 0.9 --served-model-name megrez-moe --trust_remote_code
```

ç°åœ¨ï¼Œå¯ä»¥é€šè¿‡curlå‘é€è¯·æ±‚
```shell
curl --location 'http://localhost:8000/v1/chat/completions' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer sk-123456' \
--data '{
    "model": "megrez-moe",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§ï¼Ÿ"
                }
            ]
        }
    ]
}'
```

### SGLang

æ¨è `sglang>=0.4.9.post2` çš„ç‰ˆæœ¬
```shell
cd demo/sglang
export MODEL_PATH="sii-research/InnoMegrez2-Preview" 
python3 infer_sglang_offline.py $MODEL_PATH
```


## æœ€ä½³å®è·µ

ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œå»ºè®®ä»¥ä¸‹è®¾ç½®ï¼š

1. é‡‡æ ·å‚æ•°ï¼šæ¨èä½¿ç”¨ Temperature=0.7 å’Œ TopP=0.9 ã€‚

2. æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼šåœ¨åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æç¤ºæ¥æ ‡å‡†åŒ–æ¨¡å‹è¾“å‡ºï¼Œæ¯”å¦‚ï¼š
    * æ•°å­¦é—®é¢˜ï¼šåœ¨æç¤ºä¸­åŒ…å«â€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨\boxed{}ä¸­ã€‚â€
    * é€‰æ‹©é¢˜ï¼šåœ¨æç¤ºä¸­æ·»åŠ ä»¥ä¸‹ JSON ç»“æ„ä»¥æ ‡å‡†åŒ–å“åº”ï¼šâ€œè¯·åœ¨ answer å­—æ®µä¸­ä»…ä»¥é€‰æ‹©å­—æ¯çš„å½¢å¼æ˜¾ç¤ºæ‚¨çš„é€‰æ‹©ï¼Œä¾‹å¦‚ "answer": "C" ã€‚â€

# è®¸å¯å£°æ˜

æˆ‘ä»¬æ‰€æœ‰çš„å¼€æºæ¨¡å‹å‡é‡‡ç”¨Apache 2.0åè®®æˆæƒã€‚

# è”ç³»æˆ‘ä»¬

å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶æäº¤GitHub issueæˆ–è”ç³»[å¾®ä¿¡ç¾¤ç»„](./assets/wechat-group.jpg)ã€‚